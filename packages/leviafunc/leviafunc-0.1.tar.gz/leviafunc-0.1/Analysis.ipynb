{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP of legal texts\n",
    "Analysis of agreements between governments \n",
    "* Key words & key phrases extraction with TF-IDF and N-gramms\n",
    "* NER for DATES with ([Natasha](https://natasha.github.io/demo/) (rule-based lib for Russian language). Sequence model, implemented in [AnaGo](https://anago.herokuapp.com/) and NER by [DeepMIPT](https://demo.ipavlov.ai/) have lower accuracy for this type of text.\n",
    "* Dictionary method and morphological analysis for finding ORGANIZATIONS and COUNTRIES (accuracy is more important than the opportunity to expand the lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alissia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import docx\n",
    "from datetime import date\n",
    "import pymorphy2\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "m = pymorphy2.MorphAnalyzer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from natasha import (\n",
    "    DatesExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(document):\n",
    "    if document.endswith(\".txt\"):\n",
    "        with open(document, \"r\") as f:\n",
    "            return f.readlines()\n",
    "    elif document.endswith(\".docx\"):\n",
    "        doc = docx.Document(document)\n",
    "        fullText = []\n",
    "        for para in doc.paragraphs:\n",
    "            fullText.append(para.text)\n",
    "        text = '\\n'.join(fullText)\n",
    "        file_lines = []\n",
    "        for line in text.splitlines():\n",
    "            if line != '':\n",
    "                file_lines.append(line)\n",
    "        return file_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(word):\n",
    "    word = re.sub(\"(</?.*?>)|(<>)|(\\\\d|\\\\W)+\", '', word).lower()\n",
    "    return m.parse(word)[0].normal_form.strip()\n",
    "def preprocess(file_readed_by_lines):\n",
    "    return [[lemm(word) for word in word_tokenize(text) if ((lemm(word) not in stopWords) and len(word)>3)] for text in file_readed_by_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(line.strip() for line in open('files/RUstopwords.txt', 'r'))\n",
    "# can be expanded\n",
    "print(len(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "countries = set(line.strip().lower() for line in open(os.path.abspath('files/countries.txt'), 'r'))\n",
    "print(len(countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "organizations = set(line.strip().lower() for line in open(os.path.abspath('files/organizations.txt'), 'r'))\n",
    "# can be expanded\n",
    "print(len(organizations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus is uploaded from https://xn--80abucjiibhv9a.xn--p1ai/%D0%BC%D0%B8%D0%BD%D0%B8%D1%81%D1%82%D0%B5%D1%80%D1%81%D1%82%D0%B2%D0%BE/68/%D1%84%D0%B0%D0%B9%D0%BB/916/%D0%9C%D0%A1_%D0%9D%D0%A2%D0%A1.pdf\n",
    "# doesn't contain the test doc\n",
    "corp = 'corpus.txt'\n",
    "corpus = preprocess(getText(os.path.abspath('files/'+corp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'testdoc.docx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'testdoc.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines = getText(os.path.abspath('files/'+filename))\n",
    "doc = preprocess(doc_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing TF-IDF and extracting key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tfidf(text_lines, sep, space=\"\"):\n",
    "    data = \"\"\n",
    "    for line in text_lines:\n",
    "        data += sep.join(line) + space\n",
    "    return data\n",
    "\n",
    "#  sorts the values in the vector while preserving the column index\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key = lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=20):\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "    \n",
    "def get_keywords(doc, corpus_tfidf):\n",
    "    \n",
    "    cv = CountVectorizer(max_df=0.85, stop_words=stopWords)\n",
    "    word_count_vector = cv.fit_transform(word_tokenize(corpus_tfidf))\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    feature_names = cv.get_feature_names()\n",
    "    \n",
    "    tf_idf_vector = tfidf_transformer.transform(cv.transform([doc])) # enumerates a vector of tf-idf scores\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "    return extract_topn_from_vector(feature_names,sorted_items,20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = data_tfidf(data_tfidf(corpus, \" \", space=\" \"), \" \")\n",
    "doc_tfidf = data_tfidf(data_tfidf(doc, \" \"), \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = list(get_keywords(doc_tfidf, corpus_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_euristics = [[word for word in line if (word[-3:]==\"ция\")] for line in doc]\n",
    "for line in kw_euristics:\n",
    "    for word in line:\n",
    "        keywords.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key phrases with tf-idf using N-gramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def words_to_bigramms(text, str_bigrams = \"\"):\n",
    "    for line in text:\n",
    "        bigrams = ngrams(line,2)\n",
    "        for k1, k2 in Counter(bigrams):\n",
    "            str_bigrams += k1+ \"_\" + k2+ \"_\" + \" \"\n",
    "    return str_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_trigramms(text, str_trigrams = \"\"):\n",
    "    for line in text:\n",
    "        trigrams = ngrams(line,3)\n",
    "        for k1, k2,k3 in Counter(trigrams):\n",
    "            str_trigrams += k1+ \"_\" + k2+ \"_\" +k3 + \" \"\n",
    "    return str_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = list(get_keywords(words_to_bigramms(corpus), words_to_bigramms(doc)))\n",
    "keyphrases += list(get_keywords(words_to_trigramms(corpus), words_to_bigramms(doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Название документа: СОГЛАШЕНИЕ между Правительством Российской Федерации и Правительством Федеративной Республики Бразилии о научно-техническом сотрудничестве\n"
     ]
    }
   ],
   "source": [
    "# euristics and morphological analysis\n",
    "title = str(doc_lines[0]).strip()\n",
    "topic = title.split(' ')[-2:]   \n",
    "noun = m.parse(topic[1])[0]\n",
    "adj = m.parse(topic[0])[0].inflect({noun.tag.gender, 'sing', 'nomn'})\n",
    "print(\"\\nНазвание документа: %s%s\" % (title[0].upper(), title[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary method \n",
    "orgs = []\n",
    "orgs_ = [[word for word in line if (word in organizations)] for line in doc]\n",
    "for line in orgs_:\n",
    "    for word in line:\n",
    "        orgs.append(word)\n",
    "for word in set(orgs):\n",
    "    print(\"\\nОрганизации:\", word[0].upper()+word[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Страны: Бразилия\n"
     ]
    }
   ],
   "source": [
    "# dictionary method \n",
    "coun_euristics = []\n",
    "coun = [[word for word in line if (word in countries)] for line in doc]\n",
    "for line in coun:\n",
    "    for word in line:\n",
    "        coun_euristics.append(word)\n",
    "for word in set(coun_euristics):\n",
    "    print(\"\\nСтраны:\", word[0].upper()+word[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Вид документа: Двустороннее соглашение\n"
     ]
    }
   ],
   "source": [
    "# euristics and morphological analysis\n",
    "act_type = [\"в рамках\", \"содружества\", \"государств-участников\", \"межгосударственном\"]\n",
    "title_lemm = ' '.join([lemm(word) for word in word_tokenize(title)])\n",
    "\n",
    "for word in act_type:\n",
    "    if lemm(word) in title_lemm:\n",
    "        type = \"Многостороннее соглашение\"\n",
    "    else:\n",
    "        type = \"Двустороннее соглашение\"\n",
    "print(\"\\nВид документа: %s\" % type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Направление: Научно-техническое сотрудничество\n",
      "Область: Научно-техническое сотрудничество\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nНаправление:\", adj[0][0].upper()+adj[0][1:].lower(), lemm(noun[0]))\n",
    "print(\"Область:\", adj[0][0].upper()+adj[0][1:].lower(), lemm(noun[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Дата заключения: 1997-11-21\n",
      "\n",
      "Дата вступления в силу: 1999-09-30\n",
      "\n",
      "*Документ также содержит даты в неполном формате (2002, 12, None)\n"
     ]
    }
   ],
   "source": [
    "# NER\n",
    "dates = []\n",
    "attention = set()\n",
    "extractor = DatesExtractor()\n",
    "for line in doc_lines:\n",
    "    matches = extractor(line)\n",
    "    for index, match in enumerate(matches):\n",
    "        try:\n",
    "            dates.append(date(match.fact.year, match.fact.month, match.fact.day))\n",
    "        except TypeError as e:\n",
    "            attention = match.fact.year, match.fact.month, match.fact.day\n",
    "# usually acts with earlier dates are denied or are the ones which the current document is based on\n",
    "data2 = dates.pop(dates.index(max(dates)))\n",
    "data1 = max(dates);\n",
    "print(\"\\nДата заключения:\", data1)\n",
    "print(\"\\nДата вступления в силу:\", data2)\n",
    "if len(attention) > 0:\n",
    "    print(\"\\n*Документ также содержит даты в неполном формате\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ключевые слова:\n",
      "сотрудничество\n",
      "бразилия\n",
      "правительство\n",
      "комиссия\n",
      "реализация\n",
      "страна\n",
      "каждый\n",
      "организация\n",
      "сила\n",
      "информация\n",
      "проект\n",
      "сторона\n",
      "научнотехнический\n",
      "настоящее\n",
      "федеративный\n",
      "соглашение\n",
      "действие\n",
      "республика\n",
      "отношение\n",
      "рекомендация\n",
      "ассоциация\n",
      "координация\n",
      "настоящий\n",
      "свой\n",
      "бразилиа\n",
      "федерация\n",
      "\n",
      "Наиболее часто встречающиеся выражения (n-gramms):\n",
      "договариваться_сторона_\n",
      "научнотехнический_информация_\n",
      "сотрудничество_область_\n",
      "прекращение_действие_\n",
      "соглашение_научнотехнический_\n",
      "декабрь__\n",
      "наука_техника_\n",
      "федерация_правительство_\n",
      "область_наука_\n",
      "интеллектуальный_собственность_\n",
      "бразилия_апрель_\n",
      "рамка_настоящий_\n",
      "_год_\n",
      "правительство_российский_\n",
      "сотрудничество_рамка_\n",
      "бразилиа_ноябрь_\n",
      "российский_федерация_\n",
      "результат_сотрудничество_\n",
      "распределение_право_\n",
      "научнотехнический_сотрудничество_\n",
      "настоящий_соглашение_\n",
      "настоящее_соглашение_\n",
      "вступать_сила_\n",
      "оба_страна_\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nКлючевые слова:\")\n",
    "for word in set(keywords):\n",
    "    print(word)\n",
    "\n",
    "print(\"\\nНаиболее часто встречающиеся выражения (n-gramms):\")\n",
    "for phrase in set(keyphrases):\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
