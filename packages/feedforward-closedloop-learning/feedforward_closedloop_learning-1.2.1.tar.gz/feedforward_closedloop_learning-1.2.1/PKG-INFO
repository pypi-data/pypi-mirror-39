Metadata-Version: 1.1
Name: feedforward_closedloop_learning
Version: 1.2.1
Summary: Feedforward Closedloop Learning (FCL)
Home-page: https://github.com/glasgowneuro/feedforward_closedloop_learning
Author: Bernd Porr, Paul Miller
Author-email: bernd@glasgowneuro.tech
License: GPL 3.0
Description: =========================================
        Feedforward closed loop learning library/API
        =========================================
        
        The documentation of all functions can be obtained with::
          >>> import feedback_closedloop_learning as fcl
          >>> help(fcl)
        
        The detailed documentation can be generated by doxygen.
        
        The best way to get started is to look at the script
        in `tests_py` from:
        https://github.com/glasgowneuro/feedback_closed_loop_learning
        
        A full application using the Python API is our vizdoom
        agent: https://github.com/glasgowneuro/fcl_demos
        
        
        API
        ===
        
        The API is identical to the C++ API: `fcl.h`,
        `neuron.h` and `layer.h` contain docstrings for
        all important calls.
        
        Constructors::
        
          num_of_inputs: number of inputs in the input layer
          num_of_hidden_neurons_per_layer_array: number of neurons in each layer
          num_hid_layers: number of hidden layer (needs to match with array above)
          num_outputs: number of output in the output layer
        
          FeedforwardClosedloopLearning(
        			num_of_inputs,
        			num_of_hidden_neurons_per_layer_array,
        			_num_hid_layers,
        			num_outputs)
        
        
        
          filter number >0 means: filterbank
          filter number = 0 means layer without filters
          num_of_inputs: number of inputs in the input layer
          num_of_hidden_neurons_per_layer_array: number of neurons in each layer
          num_hid_layers: number of hidden layer (needs to match with array above)
          num_outputs: number of output in the output layer
          num_filtersInput: number of filters at the input layer
          num_filtersHidden: number of filters in the hiddel layers (usually zero)
          minT: minimum/first temporal duration of the 1st filter
          maxT: maximum/last temporal duration of the last filter
          
          FeedforwardClosedloopLearning(
        			num_of_inputs,
        			num_of_hidden_neurons_per_layer_array,
        			num_hid_layers,
        			num_outputs,
        			num_filtersInput,
        			num_filtersHidden,
        			minT,
        			maxT)
        
        			
        Performs the simulation step::
        
          input: Array with the input values
          error: Array of the error signals
        
          doStep(input, error)
        
          
        Gets the output from one of the output neurons::
        
          double getOutput(index)
        
          
        Sets globally the learning rate::
        
          learningRate: for all layers and neurons
          
          setLearningRate(learningRate)
        
          
        Sets how the learnign rate increases or decreases from layer to layer::
        
          learningRateDiscountFactor: >1 means higher learning rate in deeper layers
        
          setLearningRateDiscountFactor(learningRateDiscountFactor)
        
          
        Sets a typical weight decay scaled with the learning rate::
        
          decay: >0, the larger the faster the decay
          
          setDecay(double decay)
        
          
        Sets the global momentum for all layers::
        
          setMomentum(double momentum)
        
          
        Sets the activation function of the Neuron::
        
          activationFunction: see Neuron.ActivationFunction for the different options
          
          setActivationFunction(activationFunction);
        
          
        Inits the weights in all layers::
        
          max: Maximum value of the weights
          initBias: If the bias also should be initialised
          weightInitMethod: see Neuron::WeightInitMethod for the options
          
          initWeights(max = 0.001,
                      initBias = 1,
                      weightInitMethod = Neuron.MAX_OUTPUT_RANDOM);
        
        		    
        Seeds the random number generator::
        
          seedRandom(s)
        
        	
        Sets globally the bias::
        
          setBias(bias);
        
        	
        Returns the number of hidden layers::
        
          getNumHidLayers()
        
        	
        Gets the total number of layers::
        
          getNumLayers()
        
          
        Gets a pointer to a layer::
        
          getLayer(i)
        
          
        Gets the output layer::
        
          getOutputLayer()
        
          
        Returns all Layers::
        
          getLayers()
        
          
        Saves the whole network::
        
          bool saveModel(const char* name);
        
          
        Loads the network::
        
          bool loadModel(const char* name);
        
Platform: UNKNOWN
Classifier: Intended Audience :: Developers
Classifier: Operating System :: POSIX
Classifier: Programming Language :: Python
