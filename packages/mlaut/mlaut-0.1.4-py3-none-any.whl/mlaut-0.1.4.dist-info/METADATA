Metadata-Version: 2.1
Name: mlaut
Version: 0.1.4
Summary: Automation of large-scale training, evaluation and benchmarking of machine learning algorithms.
Home-page: https://viktorkaz.github.io/
Author: Viktor Kazakov
Author-email: viktor.kazakov.18@ucl.ac.uk
License: BDS
Platform: UNKNOWN
Requires-Dist: Orange3 (>=3.18.0)
Requires-Dist: h5py (>=2.8pandas>=0.21)
Requires-Dist: matplotlib (>=2.1.0)
Requires-Dist: numpy (>=1.15.4)
Requires-Dist: scikit-learn (>=0.20.1)
Requires-Dist: scikit-posthocs (>=0.3.4)
Requires-Dist: scipy (>=1.0.0)
Requires-Dist: tables (>=3.4.2)
Requires-Dist: tensorflow (>=1.12.0)
Requires-Dist: wrapt (>=1.10.11)

MLAUT is a modelling and workflow toolbox in python, written with the aim of simplifying large scale benchmarking of machine learning strategies, e.g., validation, evaluation and comparison with respect to predictive/task-specific performance or runtime. Key features are:

* automation of the most common workflows for benchmarking modelling strategies on multiple datasets including statistical post-hoc analyses, with user-friendly default settings.

* unified interface with support for scikit-learn strategies, keras deep neural network architectures, including easy user extensibility to (partially or completely) custom strategies.

* higher-level meta-data interface for strategies, allowing easy specification of scikit-learn pipelines and keras deep network architectures, with user-friendly (sensible) default configurations

* easy setting up and loading of data set collections for local use (e.g., data frames from local memory, UCI repository, openML, Delgado study, PMLB).

* back-end agnostic, automated local file system management of datasets, fitted models, predictions, and results, with the ability to easily resume crashed benchmark experiments with long running times.

