%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}


% Jupyter Notebook prompt colors
\definecolor{nbsphinxin}{HTML}{303F9F}
\definecolor{nbsphinxout}{HTML}{D84315}
% ANSI colors for output streams and traceback highlighting
\definecolor{ansi-black}{HTML}{3E424D}
\definecolor{ansi-black-intense}{HTML}{282C36}
\definecolor{ansi-red}{HTML}{E75C58}
\definecolor{ansi-red-intense}{HTML}{B22B31}
\definecolor{ansi-green}{HTML}{00A250}
\definecolor{ansi-green-intense}{HTML}{007427}
\definecolor{ansi-yellow}{HTML}{DDB62B}
\definecolor{ansi-yellow-intense}{HTML}{B27D12}
\definecolor{ansi-blue}{HTML}{208FFB}
\definecolor{ansi-blue-intense}{HTML}{0065CA}
\definecolor{ansi-magenta}{HTML}{D160C4}
\definecolor{ansi-magenta-intense}{HTML}{A03196}
\definecolor{ansi-cyan}{HTML}{60C6C8}
\definecolor{ansi-cyan-intense}{HTML}{258F8F}
\definecolor{ansi-white}{HTML}{C5C1B4}
\definecolor{ansi-white-intense}{HTML}{A1A6B2}
\definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
\definecolor{ansi-default-inverse-bg}{HTML}{000000}

% Define "notice" environment, which was removed in Sphinx 1.7.
% At some point, "notice" should be replaced by "sphinxadmonition",
% which is available since Sphinx 1.5.
\makeatletter
\@ifundefined{notice}{%
\newenvironment{notice}{\begin{sphinxadmonition}}{\end{sphinxadmonition}}%
}{}
\makeatother



\title{sng Documentation}
\date{Dec 05, 2018}
\release{0.3}
\author{Alexander Engelhardt}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\maketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


The \sphinxcode{\sphinxupquote{sng}} package (short for “Startup Name Generator”) is hosted \sphinxhref{https://github.com/AlexEngelhardt/startup-name-generator}{on GitHub}. This is its documentation.


\chapter{Introduction}
\label{\detokenize{introduction:introduction}}\label{\detokenize{introduction::doc}}

\section{Summary}
\label{\detokenize{introduction:summary}}
This package takes a wordlist, then trains a model that can automatically
generate name suggestions for things like companies or software. You feed it a
text corpus with a certain theme, e.g. a Celtic text, and it then outputs
similar sounding suggestions. An example call to a trained model looks like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cfg} \PYG{o}{=} \PYG{n}{sng}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}\PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ Labs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{gen} \PYG{o}{=} \PYG{n}{sng}\PYG{o}{.}\PYG{n}{Generator}\PYG{p}{(}\PYG{n}{wordlist\PYGZus{}file}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}wordlist.txt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{gen}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{gen}\PYG{o}{.}\PYG{n}{simulate}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}

\PYG{g+go}{[\PYGZsq{}Ercos Software\PYGZsq{}, \PYGZsq{}Riuri Software\PYGZsq{}, \PYGZsq{}Palia Software\PYGZsq{},}
\PYG{g+go}{ \PYGZsq{}Critim Software\PYGZsq{}]}
\end{sphinxVerbatim}

The package source is available on \sphinxhref{https://github.com/AlexEngelhardt/startup-name-generator}{GitHub}.

I also gave a lightning talk presenting the basic idea, it’s available \sphinxhref{https://www.youtube.com/watch?v=1w3Q3CEldG0}{on Youtube}.


\section{Supplied wordlists}
\label{\detokenize{introduction:supplied-wordlists}}
The package comes with sample word lists in German, English, and French, and
more “exotic” corpora of Pokemon names, death metal song lyrics, and
J.R.R. Tolkien’s Black Speech, the language of Mordor. Below, I’ll briefly
describe them and also show some randomly sampled output for the (totally not
cherry-picked) generated words. These corpora are available in the \sphinxhref{https://github.com/AlexEngelhardt/startup-name-generator/tree/master/sng/wordlists}{wordlists
subdirectory}.
\begin{itemize}
\item {} 
\sphinxhref{http://gutenberg.spiegel.de/buch/robinson-crusoe-747/1}{German}.
The first chapter of Robinson Crusoe in German

\item {} 
\sphinxhref{http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt}{English}.
Alice in Wonderland

\item {} 
\sphinxhref{http://ntwords.com/eng\_gr.htm}{Greek}
A short list of Greek words (in the latin alphabet)

\item {} 
Gallic \sphinxhref{http://oda.chez-alice.fr/gallicdico.htm}{(source 1)}: A list of Gallic words. \sphinxhref{http://www.darklyrics.com/e/eluveitie.html}{(source 2)}: Selected Gallic song lyrics from the band Eluveitie

\item {} 
\sphinxhref{http://www.thelatinlibrary.com/ovid/ovid.met1.shtml}{Latin}:
The first book of Ovid’s Metamorphoses

\item {} 
\sphinxhref{https://fr.wikipedia.org/wiki/France}{French}:
The French Wikipedia entry for France

\item {} 
\sphinxhref{http://www.darklyrics.com/b/behemoth.html}{Behemoth}:
English song lyrics by the death metal band Behemoth. Sampled words will have be more occult themed

\item {} 
\sphinxhref{http://www.angelfire.com/ia/orcishnations/englishorcish.html}{The Black Speech}:
JRR Tolkien’s language of the Orcs

\item {} 
\sphinxhref{http://www.lipsum.com}{Lorem Ipsum}:
The classic lorem ipsum text

\item {} 
\sphinxhref{https://github.com/veekun/pokedex/blob/74e22520db7e6706d2e7ad2109f15b7e9be10a24/pokedex/data/csv/pokemon.csv}{Pokemon}:
A list of 900 Pokemon. Your company will sound like one of them, then!

\end{itemize}


\subsection{Celtic}
\label{\detokenize{introduction:celtic}}
My main target was a Celtic sounding name. Therefore, I first created a corpus of two parts (\sphinxhref{https://github.com/AlexEngelhardt/startup-name-generator/blob/master/sng/wordlists/gallic.txt}{browse it here}): first, a Gallic dictionary, and second, selected song lyrics by the swiss band \sphinxhref{http://www.darklyrics.com/e/eluveitie.html}{Eluveitie}. They write in the Gaulish language, which reads very pleasantly and makes for good startup names in my opinion:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Lucia}
\PYG{n}{Reuoriosi}
\PYG{n}{Iacca}
\PYG{n}{Helvetia}
\PYG{n}{Eburo}
\PYG{n}{Ectros}
\PYG{n}{Uxopeilos}
\PYG{n}{Etacos}
\PYG{n}{Neuniamins}
\PYG{n}{Nhellos}
\end{sphinxVerbatim}


\subsection{Pokemon}
\label{\detokenize{introduction:id1}}
I also wanted to feed the model a \sphinxhref{https://github.com/AlexEngelhardt/startup-name-generator/blob/master/sng/wordlists/pokemon.txt}{list of all Pokemon}, and then generate a list of new Pokemon-themed names:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Grubbin}
\PYG{n}{Agsharon}
\PYG{n}{Oricorina}
\PYG{n}{Erskeur}
\PYG{n}{Electrode}
\PYG{n}{Ervivare}
\PYG{n}{Unfeon}
\PYG{n}{Whinx}
\PYG{n}{Onterdas}
\PYG{n}{Cagbanitl}
\end{sphinxVerbatim}


\subsection{Tolkien’s Black Speech}
\label{\detokenize{introduction:tolkiens-black-speech}}
J.R.R. Tolkien’s \sphinxhref{http://www.angelfire.com/ia/orcishnations/englishorcish.html}{Black Speech}, the language of the Orcs, was a just-for-fun experiment (\sphinxhref{https://github.com/AlexEngelhardt/startup-name-generator/blob/master/sng/wordlists/black-speech.txt}{wordlist here}). It would be too outlandish for a company name, but nonetheless an interesting sounding corpus:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Aratani}
\PYG{n}{Arau}
\PYG{n}{Ushtarak}
\PYG{n}{Ishi}
\PYG{n}{Kakok}
\PYG{n}{Ulig}
\PYG{n}{Ruga}
\PYG{n}{Arau}
\PYG{n}{Lakan}
\PYG{n}{Udaneg}
\end{sphinxVerbatim}


\subsection{Death metal lyrics}
\label{\detokenize{introduction:death-metal-lyrics}}
As a metal fan, I also wanted to see what happens if the training data becomes song lyrics. I used lyrics by the Polish death metal band \sphinxhref{http://www.darklyrics.com/b/behemoth.html}{Behemoth}, because the songs are filled with occult-sounding words (\sphinxhref{https://github.com/AlexEngelhardt/startup-name-generator/blob/master/sng/wordlists/behemoth.txt}{see the wordlist}):

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Artered}
\PYG{n}{Unlieling}
\PYG{n}{Undewfions}
\PYG{n}{Archon}
\PYG{n}{Unleash}
\PYG{n}{Architer}
\PYG{n}{Archaror}
\PYG{n}{Lament}
\PYG{n}{Unionih}
\PYG{n}{Lacerate}
\end{sphinxVerbatim}

You can add anything from “Enterprises” to “Labs” as a suffix to your company name. I found a long list of possible suffixes \sphinxhref{https://www.reddit.com/r/Entrepreneur/comments/4jfrgl/is\_there\_a\_list\_of\_generic\_company\_name\_endings/}{here}.


\section{Background}
\label{\detokenize{introduction:background}}

\subsection{My need for automatic company names}
\label{\detokenize{introduction:my-need-for-automatic-company-names}}
Recently, an associate and I started work on founding a software development
company. The one thing we struggled most with was to come up with a good
name. It has to sound good, be memorable, and the domain should still be
available. Both of us like certain themes, e.g. words from Celtic
languages. Sadly, most actual celtic words were already in use. We’d come up
with a nice name every one or two days, only to find out that there’s an
\sphinxhref{https://www.google.de/search?channel=fs\&q=camox}{HR company and a ski model with that exact name}.

We needed a larger number of candidate names, and manual selection took too
long. I came up with an idea for a solution: Create a neural network and have it
generate new, artificial words that hopefully are not yet in use by other
companies. You’d feed it a corpus of sample words in a certain style you
like. For example, Celtic songs, or a Greek dictionary, or even a list of
Pokemon. If you train the model on the character-level text, it should pick up
the peculiarities of the text (the “language”) and then be able to sample new
similar sounding words.

A famous \sphinxhref{http://karpathy.github.io/2015/05/21/rnn-effectiveness/}{blog post by Andrej Karpathy} provided me with the necessary knowledge
and the confidence that this is a realistic idea. In his post, he uses recurrent
neural networks (RNNs) to generate Shakespeare text, Wikipedia articles, and
(sadly, non-functioning) source code. Thus, my goal of generating single words
should not be a big problem.


\chapter{Installation}
\label{\detokenize{installation:installation}}\label{\detokenize{installation::doc}}

\section{Install from GitHub}
\label{\detokenize{installation:install-from-github}}
Clone the repository and install the package:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{git} \PYG{n}{clone} \PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{github}\PYG{o}{.}\PYG{n}{com}\PYG{o}{/}\PYG{n}{AlexEngelhardt}\PYG{o}{/}\PYG{n}{startup}\PYG{o}{\PYGZhy{}}\PYG{n}{name}\PYG{o}{\PYGZhy{}}\PYG{n}{generator}\PYG{o}{.}\PYG{n}{git}
\PYG{n}{cd} \PYG{n}{startup}\PYG{o}{\PYGZhy{}}\PYG{n}{name}\PYG{o}{\PYGZhy{}}\PYG{n}{generator}
\PYG{n}{python} \PYG{n}{setup}\PYG{o}{.}\PYG{n}{py} \PYG{n}{install}
\end{sphinxVerbatim}

I think this also works:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{upgrade} \PYG{n}{git}\PYG{o}{+}\PYG{n}{git}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{github}\PYG{o}{.}\PYG{n}{com}\PYG{o}{/}\PYG{n}{AlexEngelhardt}\PYG{o}{/}\PYG{n}{startup}\PYG{o}{\PYGZhy{}}\PYG{n}{name}\PYG{o}{\PYGZhy{}}\PYG{n}{generator}\PYG{o}{.}\PYG{n}{git}
\end{sphinxVerbatim}


\section{Install from PyPI}
\label{\detokenize{installation:install-from-pypi}}
Just issue:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{sng}
\end{sphinxVerbatim}

I am still working on making this package available on PyPI though.


\chapter{Quickstart}
\label{\detokenize{notebooks/01_quickstart:Quickstart}}\label{\detokenize{notebooks/01_quickstart::doc}}
\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [1]: }\PYG{o}{\PYGZpc{}}\PYG{k}{load\PYGZus{}ext} autoreload
        \PYG{o}{\PYGZpc{}}\PYG{k}{autoreload} 2
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [2]: }\PYG{c+c1}{\PYGZsh{} While the sng package is not installed, add the package\PYGZsq{}s path}
        \PYG{c+c1}{\PYGZsh{} (the parent directory) to the library path:}
        
        \PYG{k+kn}{import} \PYG{n+nn}{os}
        \PYG{k+kn}{import} \PYG{n+nn}{sys}
        \PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{abspath}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../../}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [3]: }\PYG{k+kn}{import} \PYG{n+nn}{sng}
\end{Verbatim}



%
\begin{OriginalVerbatim}[commandchars=\\\{\}]
Using TensorFlow backend.
\end{OriginalVerbatim}
% The following \relax is needed to avoid problems with adjacent ANSI
% cells and some other stuff (e.g. bullet lists) following ANSI cells.
% See https://github.com/sphinx-doc/sphinx/issues/3594
\relax


\section{Prepare and train the model}
\label{\detokenize{notebooks/01_quickstart:Prepare-and-train-the-model}}
Create a Config object to set your own preferences regarding training or
simulation:

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [4]: }\PYG{n}{cfg} \PYG{o}{=} \PYG{n}{sng}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}
            \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{50}
        \PYG{p}{)}
        \PYG{n}{cfg}\PYG{o}{.}\PYG{n}{to\PYGZus{}dict}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxout}{Out[4]: }\PYGZob{}\PYGZsq{}batch\PYGZus{}size\PYGZsq{}: 64,
         \PYGZsq{}debug\PYGZsq{}: True,
         \PYGZsq{}epochs\PYGZsq{}: 50,
         \PYGZsq{}hidden\PYGZus{}dim\PYGZsq{}: 50,
         \PYGZsq{}max\PYGZus{}word\PYGZus{}len\PYGZsq{}: 12,
         \PYGZsq{}min\PYGZus{}word\PYGZus{}len\PYGZsq{}: 4,
         \PYGZsq{}n\PYGZus{}layers\PYGZsq{}: 2,
         \PYGZsq{}suffix\PYGZsq{}: \PYGZsq{}\PYGZsq{},
         \PYGZsq{}temperature\PYGZsq{}: 1.0,
         \PYGZsq{}verbose\PYGZsq{}: True\PYGZcb{}
\end{Verbatim}

Choose from one of these builtin wordlists to get started quickly:

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [5]: }\PYG{n}{sng}\PYG{o}{.}\PYG{n}{show\PYGZus{}builtin\PYGZus{}wordlists}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxout}{Out[5]: }[\PYGZsq{}gallic.txt\PYGZsq{},
         \PYGZsq{}english.txt\PYGZsq{},
         \PYGZsq{}behemoth.txt\PYGZsq{},
         \PYGZsq{}lorem\PYGZhy{}ipsum.txt\PYGZsq{},
         \PYGZsq{}greek.txt\PYGZsq{},
         \PYGZsq{}black\PYGZhy{}speech.txt\PYGZsq{},
         \PYGZsq{}german.txt\PYGZsq{},
         \PYGZsq{}french.txt\PYGZsq{},
         \PYGZsq{}latin.txt\PYGZsq{},
         \PYGZsq{}pokemon.txt\PYGZsq{}]
\end{Verbatim}

We’ll load the latin wordlist and look at a few sample words:

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [6]: }\PYG{n}{latin} \PYG{o}{=} \PYG{n}{sng}\PYG{o}{.}\PYG{n}{load\PYGZus{}builtin\PYGZus{}wordlist}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{latin.txt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [7]: }\PYG{n}{latin}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxout}{Out[7]: }[\PYGZsq{}in\PYGZsq{}, \PYGZsq{}nova\PYGZsq{}, \PYGZsq{}fert\PYGZsq{}, \PYGZsq{}animus\PYGZsq{}, \PYGZsq{}mutatas\PYGZsq{}]
\end{Verbatim}

Initialize and fit the word generator:

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [8]: }\PYG{n}{gen} \PYG{o}{=} \PYG{n}{sng}\PYG{o}{.}\PYG{n}{Generator}\PYG{p}{(}\PYG{n}{wordlist}\PYG{o}{=}\PYG{n}{latin}\PYG{p}{,} \PYG{n}{config}\PYG{o}{=}\PYG{n}{cfg}\PYG{p}{)}
\end{Verbatim}



%
\begin{OriginalVerbatim}[commandchars=\\\{\}]
2973 words

24 characters, including the \textbackslash{}n:
['\textbackslash{}n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z']

First two sample words:
['detque\textbackslash{}n', 'concordia\textbackslash{}n']
\end{OriginalVerbatim}
% The following \relax is needed to avoid problems with adjacent ANSI
% cells and some other stuff (e.g. bullet lists) following ANSI cells.
% See https://github.com/sphinx-doc/sphinx/issues/3594
\relax

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [9]: }\PYG{n}{gen}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}



%
\begin{OriginalVerbatim}[commandchars=\\\{\}]
epoch 0 words: Rcqeslrot, Hqvqiie, Ntyfuamiie, Fvgseafueiaa, loss: 1.5849
epoch 10 words: Uutsque, Duluas, Epeenunt, Omanetas, loss: 1.2277
epoch 20 words: Eibosque, Iuritas, Xoncut, Omniditus, loss: 1.1217
epoch 30 words: Lirus, Timone, Lilosidum, Oggo, loss: 1.0706
epoch 40 words: Unga, Oricuva, Umnaras, Untit, loss: 1.029
\end{OriginalVerbatim}
% The following \relax is needed to avoid problems with adjacent ANSI
% cells and some other stuff (e.g. bullet lists) following ANSI cells.
% See https://github.com/sphinx-doc/sphinx/issues/3594
\relax

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [10]: }\PYG{n}{gen}\PYG{o}{.}\PYG{n}{simulate}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxout}{Out[10]: }[\PYGZsq{}Baleve\PYGZsq{}, \PYGZsq{}Remiduitum\PYGZsq{}, \PYGZsq{}Urbam\PYGZsq{}, \PYGZsq{}Ugnue\PYGZsq{}]
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [11]: }\PYG{n}{gen}\PYG{o}{.}\PYG{n}{config}\PYG{o}{.}\PYG{n}{suffix} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ Software}\PYG{l+s+s1}{\PYGZsq{}}
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [12]: }\PYG{n}{gen}\PYG{o}{.}\PYG{n}{simulate}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxout}{Out[12]: }[\PYGZsq{}Otimanae Software\PYGZsq{}, \PYGZsq{}Repte Software\PYGZsq{}, \PYGZsq{}Redeps Software\PYGZsq{}, \PYGZsq{}Urque Software\PYGZsq{}]
\end{Verbatim}


\section{Save and load the model for later}
\label{\detokenize{notebooks/01_quickstart:Save-and-load-the-model-for-later}}
\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [14]: }\PYG{n}{gen}\PYG{o}{.}\PYG{n}{save}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{overwrite}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{Verbatim}

Then:

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [15]: }\PYG{n}{gen2} \PYG{o}{=} \PYG{n}{sng}\PYG{o}{.}\PYG{n}{Generator}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{Verbatim}



%
\begin{OriginalVerbatim}[commandchars=\\\{\}]
2973 words

24 characters, including the \textbackslash{}n:
['\textbackslash{}n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z']

First two sample words:
['demittere\textbackslash{}n', 'sonanti\textbackslash{}n']
\end{OriginalVerbatim}
% The following \relax is needed to avoid problems with adjacent ANSI
% cells and some other stuff (e.g. bullet lists) following ANSI cells.
% See https://github.com/sphinx-doc/sphinx/issues/3594
\relax

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxin}{In [16]: }\PYG{n}{gen2}\PYG{o}{.}\PYG{n}{simulate}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\end{Verbatim}

\fvset{hllines={, ,}}%
\begin{Verbatim}[commandchars=\\\{\}]
\textcolor{nbsphinxout}{Out[16]: }[\PYGZsq{}Redent Software\PYGZsq{}, \PYGZsq{}Ulde Software\PYGZsq{}, \PYGZsq{}Uxsit Software\PYGZsq{}, \PYGZsq{}Ortitum Software\PYGZsq{}]
\end{Verbatim}


\chapter{Discussion}
\label{\detokenize{discussion:discussion}}\label{\detokenize{discussion::doc}}

\section{Data preprocessing}
\label{\detokenize{discussion:data-preprocessing}}
For input data, I just built a corpus by using raw, copy-pasted text that
sometimes included numbers and other symbols. A preprocessing was definitely
necessary. I first stripped out all non-letter characters (keeping
language-specific letters such as German umlauts). Then, I split the text up in
words and reduced the corpus to keep only unique words, i.e. one copy of each
word. I figured this step was reasonable since I did not want the model to learn
the most common words, but instead to get an understanding of the entire corpus’
structure.

After this, most text corpora ended up as a list of 1000 to 2000 words.


\section{The RNN architecture}
\label{\detokenize{discussion:the-rnn-architecture}}
The question which type of neural network to use was easily answered. Recurrent neural networks can model language particularly well, and were the appropriate type for this task of word generation.

However, to my knowledge, finding the ‘perfect’ RNN architecture is still somewhat of a black art. Questions like how many layers, how many units, and how many epochs have no definite answer, but rely on experience, intuition, and sometimes just brute force.

I wanted a model that was as complex as necessary, but as simple as possible. This would save training time. After some experiments, I settled for a two-layer LSTM 50 units each, training it for 500 epochs and a batch size of 64 words. The words this model outputs sound good enough that I didn’t put any more energy in fine-tuning the architecture.


\section{Sampling Temperature}
\label{\detokenize{discussion:sampling-temperature}}
The RNN generates a new name character by character. In particular, at any given
step, it does not just output a character, but the distribution for the next
character. This allows us to pick the letter with the highest probability, or
sample from the provided distribution.

A nice touch I found is to vary the \sphinxhref{https://cs.stackexchange.com/questions/79241/what-is-temperature-in-lstm-and-neural-networks-generally}{temperature} of the sampling procedure. The
temperature is a parameter that adapts the weights to sample from. The
“standard” temperature 1 does not change the weights. For a low temperature,
trending towards zero, the sampling becomes less random and the letter
corresponding to the maximum weight is chosen almost always. The other extreme,
a large temperature trending towards infinity, will adjust the weights to a
uniform distribution, representing total randomness. You can lower the
temperature to get more conservative samples, or raise it to generate more
random words. For actual text sampling, a temperature below 1 might be
appropriate, but since I wanted new words, a higher temperature seemed better.

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{temperature}.png}

In the image above, imagine we want to sample one letter from A, B, …, J. Your
RNN might output the weights represented by the red bars. You’d slightly favor
A, E, G, H, and J there. Now if you transform these weights with a very cold
temperature (see the yellow-ish bars), your model gets more conservative,
sticking to the argmax letter(s). In this case, you’d most likely get one letter
of E, G, and H. If you lower the temperature even further, your sampling will
always return the argmax letter, in this case, a G.

Alternatively, you can raise the temperature. In the image above, I plotted
green bars, representing a transformation applied with a temperature of 3. You
can still see the same preferences for E, G, and H, but the magnitude of the
differences is much lower now, resulting in a more random sampling, and
consecutively, in more random names. The extreme choice of a temperature
approaching infinity would result in a totally random sampling, which then would
make all your RNN training useless, of course. There is a sweet spot for the
temperature somewhere, which you have to discover by trial-and-error.


\chapter{Modules}
\label{\detokenize{modules:modules}}\label{\detokenize{modules::doc}}

\section{Config}
\label{\detokenize{modules:module-sng.Config}}\label{\detokenize{modules:config}}\index{sng.Config (module)@\spxentry{sng.Config}\spxextra{module}}
The Config module. It defines the Config class.
\index{Config (class in sng.Config)@\spxentry{Config}\spxextra{class in sng.Config}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{sng.Config.}}\sphinxbfcode{\sphinxupquote{Config}}}{\emph{**kwargs}}{}
Configuration options for model training and name generation
\begin{description}
\item[{**kwargs:}] \leavevmode
Keyword arguments that will overwrite the default config options.

\end{description}

To create a Config object that results in simulating names between
6 and 10 letters:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cfg} \PYG{o}{=} \PYG{n}{sng}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}
    \PYG{n}{min\PYGZus{}word\PYGZus{}len}\PYG{o}{=}\PYG{l+m+mi}{6}\PYG{p}{,}
    \PYG{n}{max\PYGZus{}word\PYGZus{}len}\PYG{o}{=}\PYG{l+m+mi}{10}
\PYG{p}{)}
\end{sphinxVerbatim}

To quickly inspect all values:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cfg}\PYG{o}{.}\PYG{n}{to\PYGZus{}dict}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\index{batch\_size (sng.Config.Config attribute)@\spxentry{batch\_size}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.batch_size}}\pysigline{\sphinxbfcode{\sphinxupquote{batch\_size}}\sphinxbfcode{\sphinxupquote{ = None}}}
int: The batch size for training the RNN

\end{fulllineitems}

\index{debug (sng.Config.Config attribute)@\spxentry{debug}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.debug}}\pysigline{\sphinxbfcode{\sphinxupquote{debug}}\sphinxbfcode{\sphinxupquote{ = None}}}
bool: If true, methods will add some additional attributes
to a Generator object’s \sphinxcode{\sphinxupquote{debug}} dict.

\end{fulllineitems}

\index{epochs (sng.Config.Config attribute)@\spxentry{epochs}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.epochs}}\pysigline{\sphinxbfcode{\sphinxupquote{epochs}}\sphinxbfcode{\sphinxupquote{ = None}}}
int: How many epochs to train the RNN for?

\end{fulllineitems}

\index{hidden\_dim (sng.Config.Config attribute)@\spxentry{hidden\_dim}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.hidden_dim}}\pysigline{\sphinxbfcode{\sphinxupquote{hidden\_dim}}\sphinxbfcode{\sphinxupquote{ = None}}}
int: Number of hidden units per LSTM layer

\end{fulllineitems}

\index{max\_word\_len (sng.Config.Config attribute)@\spxentry{max\_word\_len}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.max_word_len}}\pysigline{\sphinxbfcode{\sphinxupquote{max\_word\_len}}\sphinxbfcode{\sphinxupquote{ = None}}}
int: How long should simulated words be maximum?

\end{fulllineitems}

\index{min\_word\_len (sng.Config.Config attribute)@\spxentry{min\_word\_len}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.min_word_len}}\pysigline{\sphinxbfcode{\sphinxupquote{min\_word\_len}}\sphinxbfcode{\sphinxupquote{ = None}}}
int: How long should simulated words be at least?

\end{fulllineitems}

\index{n\_layers (sng.Config.Config attribute)@\spxentry{n\_layers}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.n_layers}}\pysigline{\sphinxbfcode{\sphinxupquote{n\_layers}}\sphinxbfcode{\sphinxupquote{ = None}}}
int: How many LSTM layers in the model?

\end{fulllineitems}

\index{suffix (sng.Config.Config attribute)@\spxentry{suffix}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.suffix}}\pysigline{\sphinxbfcode{\sphinxupquote{suffix}}\sphinxbfcode{\sphinxupquote{ = None}}}
str: A suffix to append to the suggested names.

Choose e.g. ” Software” (with a leading space!) to see how your
company name would look with the word Software at the end.

\end{fulllineitems}

\index{temperature (sng.Config.Config attribute)@\spxentry{temperature}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.temperature}}\pysigline{\sphinxbfcode{\sphinxupquote{temperature}}\sphinxbfcode{\sphinxupquote{ = None}}}
float: Sampling temperature. Lower values are “colder”, i.e.
sampling probabilities will be more conservative.

\end{fulllineitems}

\index{to\_dict() (sng.Config.Config method)@\spxentry{to\_dict()}\spxextra{sng.Config.Config method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.to_dict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{to\_dict}}}{}{}
Convert Config object to dictionary.

\end{fulllineitems}

\index{verbose (sng.Config.Config attribute)@\spxentry{verbose}\spxextra{sng.Config.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Config.Config.verbose}}\pysigline{\sphinxbfcode{\sphinxupquote{verbose}}\sphinxbfcode{\sphinxupquote{ = None}}}
bool: If true, prints helpful messages on what is happening.

\end{fulllineitems}


\end{fulllineitems}



\section{Generator}
\label{\detokenize{modules:module-sng.Generator}}\label{\detokenize{modules:generator}}\index{sng.Generator (module)@\spxentry{sng.Generator}\spxextra{module}}\index{Generator (class in sng.Generator)@\spxentry{Generator}\spxextra{class in sng.Generator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Generator.Generator}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{sng.Generator.}}\sphinxbfcode{\sphinxupquote{Generator}}}{\emph{config=\textless{}sng.Config.Config object\textgreater{}}, \emph{wordlist\_file=None}, \emph{wordlist=None}}{}
Main class that holds the config, wordlist, and the trained model.
\begin{description}
\item[{config}] \leavevmode{[}sng.Config, optional{]}
A Config instance specifying training and simulation parameters.
If not supplied, a default configuration will be created.

\item[{wordlist\_file}] \leavevmode{[}str{]}
Path to a textfile holding the text corpus you want to use.

\item[{wordlist}] \leavevmode{[}list of strings{]}
Alternatively to \sphinxcode{\sphinxupquote{wordlist\_file}}, you can provide the already
processed wordlist, a list of (ideally unique) strings.

\end{description}
\begin{description}
\item[{config}] \leavevmode{[}sng.Config{]}
The Config object supplied, or a default object if none was supplied
at initialization.

\item[{wordlist}] \leavevmode{[}list of strings{]}
A processed list of unique words, each ending in a newline.
This is the input to the neural network.

\end{description}

You can create a word generator like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{sng}
\PYG{n}{cfg} \PYG{o}{=} \PYG{n}{sng}\PYG{o}{.}\PYG{n}{Config}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Folder for pre\PYGZhy{}installed wordlists:}
\PYG{n}{wordlist\PYGZus{}folder} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{dirname}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{abspath}\PYG{p}{(}\PYG{n}{sng}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}file\PYGZus{}\PYGZus{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wordlists}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sample\PYGZus{}wordlist} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{wordlist\PYGZus{}folder}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{latin.txt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create a Generator object with some wordlist:}
\PYG{n}{gen} \PYG{o}{=} \PYG{n}{sng}\PYG{o}{.}\PYG{n}{Generator}\PYG{p}{(}\PYG{n}{wordlist\PYGZus{}file}\PYG{o}{=}\PYG{n}{sample\PYGZus{}wordlist}\PYG{p}{,} \PYG{n}{config}\PYG{o}{=}\PYG{n}{cfg}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Train the model:}
\PYG{n}{gen}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Get a few name suggestions:}
\PYG{n}{gen}\PYG{o}{.}\PYG{n}{simulate}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}
\index{fit() (sng.Generator.Generator method)@\spxentry{fit()}\spxextra{sng.Generator.Generator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Generator.Generator.fit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit}}}{}{}
Fit the model. Adds the ‘model’ attribute to itself.

\end{fulllineitems}

\index{load() (sng.Generator.Generator class method)@\spxentry{load()}\spxextra{sng.Generator.Generator class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Generator.Generator.load}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod }}\sphinxbfcode{\sphinxupquote{load}}}{\emph{directory}}{}
Create a Generator object from a stored folder.
\begin{description}
\item[{directory}] \leavevmode{[}str{]}
Folder where you used Generator.save() to store the contents in.

\end{description}

\end{fulllineitems}

\index{save() (sng.Generator.Generator method)@\spxentry{save()}\spxextra{sng.Generator.Generator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Generator.Generator.save}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save}}}{\emph{directory}, \emph{overwrite=False}}{}
Save the model into a folder.
\begin{description}
\item[{directory}] \leavevmode{[}str{]}
The folder to store the generator in. Should be non-existing.

\item[{overwrite}] \leavevmode{[}bool{]}
If True, the folder contents will be overwritten if it already
exists. Not recommended, though.

\end{description}

\end{fulllineitems}

\index{simulate() (sng.Generator.Generator method)@\spxentry{simulate()}\spxextra{sng.Generator.Generator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.Generator.Generator.simulate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{simulate}}}{\emph{n=10}, \emph{temperature=None}, \emph{min\_word\_len=None}, \emph{max\_word\_len=None}}{}
Use the trained model to simulate a few name suggestions.
\begin{description}
\item[{n}] \leavevmode{[}int{]}
The number of name suggestions to simulate

\item[{temperature}] \leavevmode{[}float or None{]}
Sampling temperature. Lower values are “colder”, i.e.
sampling probabilities will be more conservative.
If None, will use the value specified in self.config.

\item[{min\_word\_len}] \leavevmode{[}int or None{]}
Minimum word length of the simulated names.
If None, will use the value specified in self.config.

\item[{max\_word\_len}] \leavevmode{[}int or None{]}
Maximum word length of the simulated names.
If None, will use the value specified in self.config.

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\section{Wordlists}
\label{\detokenize{modules:module-sng.wordlists.wordlists}}\label{\detokenize{modules:wordlists}}\index{sng.wordlists.wordlists (module)@\spxentry{sng.wordlists.wordlists}\spxextra{module}}\index{load\_builtin\_wordlist() (in module sng.wordlists.wordlists)@\spxentry{load\_builtin\_wordlist()}\spxextra{in module sng.wordlists.wordlists}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.wordlists.wordlists.load_builtin_wordlist}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{sng.wordlists.wordlists.}}\sphinxbfcode{\sphinxupquote{load\_builtin\_wordlist}}}{\emph{name}}{}
Load and process one of the wordlists that ship with the sng package.
\begin{description}
\item[{name}] \leavevmode{[}str{]}
A file name of one of the files in the wordlists/ directory.
Call {\hyperref[\detokenize{modules:sng.wordlists.wordlists.show_builtin_wordlists}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{show\_builtin\_wordlists()}}}}} to see a list of available
names. Choose one of these.

\end{description}

A wordlist. Literally, a list of words in the text corpus.
It’s not yet preprocessed, so there are still duplicates etc. in there.
This is taken care of by {\hyperref[\detokenize{modules:module-sng.Generator}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{sng.Generator}}}}}’s \sphinxcode{\sphinxupquote{\_\_init\_\_}} method.

\end{fulllineitems}

\index{show\_builtin\_wordlists() (in module sng.wordlists.wordlists)@\spxentry{show\_builtin\_wordlists()}\spxextra{in module sng.wordlists.wordlists}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules:sng.wordlists.wordlists.show_builtin_wordlists}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{sng.wordlists.wordlists.}}\sphinxbfcode{\sphinxupquote{show\_builtin\_wordlists}}}{}{}
Returns a list of all builtin wordlists’ filenames.

Use one of them as an argument to {\hyperref[\detokenize{modules:sng.wordlists.wordlists.load_builtin_wordlist}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{load\_builtin\_wordlist()}}}}}
and get back a ready-to-go wordlist.

\end{fulllineitems}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{s}
\item\relax\sphinxstyleindexentry{sng.Config}\sphinxstyleindexpageref{modules:\detokenize{module-sng.Config}}
\item\relax\sphinxstyleindexentry{sng.Generator}\sphinxstyleindexpageref{modules:\detokenize{module-sng.Generator}}
\item\relax\sphinxstyleindexentry{sng.wordlists.wordlists}\sphinxstyleindexpageref{modules:\detokenize{module-sng.wordlists.wordlists}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}