import gin.tf.external_configurables

tf.train.AdamOptimizer.learning_rate = 0.0005

# ================================================== #
# Parameters for A2C                                 #
# ================================================== #
AdvantageActorCriticAgent.model_fn = @build_fully_conv
AdvantageActorCriticAgent.policy_cls = @SC2MultiPolicy

AdvantageActorCriticAgent.batch_sz = 32
AdvantageActorCriticAgent.traj_len = 16

AdvantageActorCriticAgent.clip_grads_norm = 10.0
AdvantageActorCriticAgent.optimizer = @tf.train.AdamOptimizer()

AdvantageActorCriticAgent.discount = 0.99
AdvantageActorCriticAgent.gae_lambda = 0.0

AdvantageActorCriticAgent.value_coef = 0.5
AdvantageActorCriticAgent.entropy_coef = 0.001

AdvantageActorCriticAgent.bootstrap_terminals = False
AdvantageActorCriticAgent.normalize_advantages = True

# ================================================== #
# Parameters for PPO                                 #
# ================================================== #
ProximalPolicyOptimizationAgent.model_fn = @build_fully_conv
ProximalPolicyOptimizationAgent.policy_cls = @SC2MultiPolicy

ProximalPolicyOptimizationAgent.batch_sz = 64
ProximalPolicyOptimizationAgent.traj_len = 16

ProximalPolicyOptimizationAgent.n_updates = 3
ProximalPolicyOptimizationAgent.minibatch_sz = 512

ProximalPolicyOptimizationAgent.clip_grads_norm = 100.0
ProximalPolicyOptimizationAgent.optimizer = @tf.train.AdamOptimizer()

ProximalPolicyOptimizationAgent.discount = 0.99
ProximalPolicyOptimizationAgent.gae_lambda = 0.0

ProximalPolicyOptimizationAgent.value_coef = 0.5
ProximalPolicyOptimizationAgent.entropy_coef = 0.001

ProximalPolicyOptimizationAgent.bootstrap_terminals = False
ProximalPolicyOptimizationAgent.normalize_advantages = True
